{mainmatter}

# Our use case #

For better understanding we will implement a small use case in both impure and pure way. The following section will outline the specification.

# Service specification

First we need to specify the exact scope and API of our service. We'll design a service with a minimal API to keep things simple. It shall fulfil the following requirements.

The service shall provide HTTP API endpoints for:

1. the creation of a product data type identified by a unique id
2. adding translations for a product name by language code and unique id
3. returning the existing translations for a product
4. returning a list of all existing products with their translations

## Data model

We will keep the model very simple to avoid going overboard with the implementation.

1. A language code shall be defined by the ISO 639-1 (e.g. a two letter code).
2. A translation shall contain a language code and a product name (non-empty string).
3. A product shall contain a unique id (UUID version 4) and a list of translations.

## Database

The data will be stored in a relational database (RDBMS). Therefore we need to define the tables and relations within the database.

### The products table

The table `products` must contain only the unique id which is also the primary key.

### The names table

The table `names` must contain a column for the product id, one for the language code and one for the name. Its primary key is the combination of the product id and the language code. All columns must not be null. The relation to the products is realised by a foreign key constraint to the `products` table via the product id.

## HTTP API

The HTTP API shall provide the following endpoints on the given paths:

| Path              | HTTP method | Function                              |
|-------------------|-------------|---------------------------------------|
| `/products`       | POST        | Create a product.                     |
| `/products`       | GET         | Get all products and translations.    |
| `/product/{UUID}` | PUT         | Add translations.                     |
| `/product/{UUID}` | GET         | Get all translations for the product. |

The data shall be encoded in JSON using the following specification:

{caption: "JSON for a translation"}
```json
{
  "lang": "ISO-639-1 Code",
  "name": "A non empty string."
}
```

{caption: "JSON for a product"}
```json
{
  "id": "The-UUID-of-the-product",
  "names": [
    // A list of translations.
  ]
}
```

This should be enough to get us started.

# The state of the art #

Within the Scala ecosystem the Akka-HTTP library is a popular choice for implementing server side backends for HTTP APIs. Another quite popular option is the Play framework but using a full blown web framework to just provide a thin API is overkill in most cases. As most services need a database the Slick library is another popular choice which completes the picture.

However while all mentioned libraries are battle tested and proven they still have problems.

# Problems

In the domain of functional programming we want referential transparency which we will define in the following way:

I> An expression `e` is referential transparent if we can in any given program replace any occurances of `e` with the result of the evaluation of `e` without changing the behaviour of the program.

Building on that we need pure functions which are

1. only dependent on their input
2. have no side effects

This means in turn that our functions will be referential transparent.

**But**, the mentioned libraries are built upon the `Future` from Scala which uses eager evaluation and breaks referential transparency. Let's look at an example.

{caption: "Future example 1"}
```scala
import scala.concurrent.Future
import scala.concurrent.ExecutionContext.Implicits.global

for {
  _ <- Future { println("Hi there!") }
  _ <- Future { println("Hi there!") }
} yield ()
```

The code above will print the text `Hi there!` two times. But how about the following one?

{caption: "Future example 2"}
```scala
import scala.concurrent.Future
import scala.concurrent.ExecutionContext.Implicits.global

val printF = Future { println("Hi there!") }

for {
  _ <- printF
  _ <- printF
} yield ()
```

Instead of printing the text two times it will print it only once even when there is no usage of `printF` at all (try omitting the for comprehension). This means that `Future` breaks referential transparency!

Q> So `Future` makes it harder (sometimes impossible) to reason about our code and also raises the bar for testability. What can we do?

# Maybe there is another way #

If we want referential transparency, we must push the side effects to the boundaries of our system (program) which can be done by using lazy evaluation. Let's repeat the previous example in a different way.

{caption: "IO example 1"}
```scala
import cats.effect.IO
import cats.implicits._

val effect = for {
  _ <- IO(println("Hi there!"))
  _ <- IO(println("Hi there!"))
} yield ()
```

The above code will produce no output. Only if we evaluate the variable `effect` which is of type `IO[Unit]` will the output be generated (try `effect.unsafeRunSync` in the REPL). Also the second approach works like expected.

{caption: "IO example 2"}
```scala
import cats.effect.IO
import cats.implicits._

val printF = IO(println("Hi there!"))

val effect = for {
  _ <- printF
  _ <- printF
} yield ()
```

Q> What have we gained?

Suddenly we can much more easily reason about our code! And why is that? Well we don't have unexpected side effects caused by code running even when it doesn't need to. This is a sneak peak how pure code looks like. Now we only need to implement pure libraries for our use, or do we?

Luckily for us meanwhile there are several pure options available in the Scala ecosystem. We will stick to the Cats family of libraries namely http4s and Doobie as replacements for Akka-HTTP and Slick. They build upon the Cats Effect library which is an implementation of an IO monad for Scala. Some other options exist but we'll stick to the one from Cats.

To be able to contrast both ways of implementing a service we will first implement it using Akka-HTTP and Slick and will then migrate to http4s and Doobie.

# Impure implementation

We'll be using the following libraries for the impure version of the service:

1. Akka (including Akka-HTTP and Akka-Streams)
2. Slick (as database layer)
3. Flyway for database migrations (or evolutions)
4. Circe for JSON codecs and akka-http-json as wrapper
5. Refined for using refined types
6. the PostgreSQL JDBC driver

I'll spare you the sbt setup as you can look that up in the code repository (e.g. the `impure` folder in the book repo).

## Models

First we'll implement our models which are simple and straightforward. At first we need a class to store our translations or better a single translation.

```scala
final case class Translation(lang: String, name: String)
```

Q> So what is wrong with that approach?

Technically it is okay but we have a bad feeling about it. Using `Option[String]` is of no use because both fields have to be set. But a `String` can always be `null` and contain a lot of unexpected stuff (literally anything).

I> This is the moment when refined types come to you rescue!

So let us define some refined types which we can use later on. At first we need a language code which obeys the restrictions of ISO-639-1 and we need a stronger definition for a product name. For the former we use a regular expression and for the latter we simply expect a string which is not empty.

{caption: "Refined types for models"}
```scala
type LanguageCode = String Refined MatchesRegex[W.`"^[a-z]{2}$"`.T]
type ProductName = String Refined NonEmpty
```

Now we can give our translation model another try.

{caption: "Translation model using refined types"}
```scala
final case class Translation(lang: LanguageCode, name: ProductName)
```

Much better and while we're at it we can also write the JSON codecs using the refined module of the Circe library. We put them into the companion object of the model.

```scala
object Translation {
  implicit val decode: Decoder[Translation] =
    Decoder.forProduct2("lang", "name")(Translation.apply)

  implicit val encode: Encoder[Translation] =
    Encoder.forProduct2("lang", "name")(t => (t.lang, t.name))
}
```

Now onwards to the product model. Because we already know of refined types we can use them from start here.

```scala
type ProductId = String Refined Uuid
final case class Product(id: ProductId, names: List[Translation])
```

Q> Now what is wrong about this?

If we look closely we realise that a `List` maybe empty. Which is valid for the list but not for our product because we need at least one entry. Luckily for us the Cats library has us covered with the `NonEmptyList` data type. Including the JSON codecs this leads us to our final implementation.
Last but not least we really should be using the existing `UUID` data type instead of rolling our own refined string version - even when it is cool. ;-)

{caption: "Product model using UUID type and NeL"}
```scala
type ProductId = java.util.UUID
final case class Product(id: ProductId, names: NonEmptyList[Translation])

object Product {
  implicit val decode: Decoder[Product] =
    Decoder.forProduct2("id", "names")(Product.apply)

  implicit val encode: Encoder[Product] =
    Encoder.forProduct2("id", "names")(p => (p.id, p.names))
}
```

We kept the type name `ProductId` by using a type alias. This is convenient but remember that a type alias does not add extra type safety (e.g.Â `type Foo = String` will be a `String`).

Q> Why do I have a bad feeling about this?

Well, maybe because a list may contain duplicate entries but the database will surely not because of unique constraints! So, let's switch to a `NonEmptySet` which is also provided by Cats.

{caption: "Product model using UUID and NeS"}
```scala
type ProductId = java.util.UUID
final case class Product(id: ProductId, names: NonEmptySet[Translation])
```

Now we have the models covered and can move on to the database layer.

## Database layer

The database layer should provide a programmatic access to the database but also should it manage changes in the database. The latter one is called migrations or evolutions. From the available options we chose Flyway as the tool to manage our database schema.

### Migrations

Flyway uses raw SQL scripts which have to be put into a certain location being `/db/migration` (under the `resources` folder) in our case. Also the files have to be named like `VXX__some_name.sql` (`XX` being a number) starting with `V1`. Please note that there are two underscores between the version prefix and the rest of the name! Because our database schema is very simply we're done quickly:

{caption: "Flyway migration for creating the database"}
```sql
CREATE TABLE "products" (
  "id" UUID NOT NULL,
  CONSTRAINT "products_pk" PRIMARY KEY ("id")
);

CREATE TABLE "names" (
  "product_id" UUID       NOT NULL,
  "lang_code"  VARCHAR(2) NOT NULL,
  "name"       TEXT       NOT NULL,
  CONSTRAINT "names_pk" 
    PRIMARY KEY ("product_id", "lang_code"),
  CONSTRAINT "names_product_id_fk" 
    FOREIGN KEY ("product_id") 
    REFERENCES "products" ("id") 
    ON DELETE CASCADE ON UPDATE CASCADE
);
```

In the code you'll see that we additionally set comments which are omitted from the code snippet above. This might be overkill here but it is a very handy feature to have and I advice you to use it for more complicated database schemas. Because the right comment (read *information*) in the right place might save a lot of time when trying to understand things.

Next we move on to the programmatic part which at first needs a configuration of our database connection. With Slick you have a multitude of options but we'll use the "Typesafe Config"[^11] approach.

{caption: "Database configuration in application.conf"}
```text
database {
  profile = "slick.jdbc.PostgresProfile$"
  db {
    connectionPool = "HikariCP"
    dataSourceClass = "org.postgresql.ds.PGSimpleDataSource"
    properties {
      serverName = "localhost"
      portNumber = "5432"
      databaseName = "impure"
      user = "impure"
      password = "secret"
    }
    numThreads = 10
  }
}
```

After we have this in place we can run the migrations via the API of Flyway. For this we have to load the configuration (we do it by creating an actor system), extract the needed information and create a JDBC url and use that with username and password to obtain a Flyway instance. On that one we simply call the method `migrate()` which will do the right thing. Basically it will check if the schema exists and decide to either create it, apply pending migrations or simply do nothing. The method will return the number of applied migrations.

{caption: "Apply database migrations via Flyway"}
```scala
implicit val system: ActorSystem    = ActorSystem()
implicit val mat: ActorMaterializer = ActorMaterializer()
implicit val ec: ExecutionContext   = system.dispatcher

val url = "jdbc:postgresql://" +
system.settings.config.getString("database.db.properties.serverName") +
":" + system.settings.config.getString("database.db.properties.portNumber") +
"/" + system.settings.config.getString("database.db.properties.databaseName")
val user = system.settings.config.getString("database.db.properties.user")
val pass = system.settings.config.getString("database.db.properties.password")
val flyway = Flyway.configure().dataSource(url, user, pass).load()
val _ = flyway.migrate()
```

Let us continue to dive into the Slick table definitions.

### Slick tables

Slick offers several options for approaching the database. For our example we will be using the lifted embedding but if needed Slick also provides the ability to perform plain SQL queries.
For the lifted embedding we have to define out tables in a way Slick can understand. While this can be tricky under certain circumstances our simple model is straightforward to implement.

{caption: "Slick product table definition"}
```scala
final class Products(tag: Tag) extends Table[(UUID)](tag, "products") {
  def id = column[UUID]("id", O.PrimaryKey)

  def * = (id)
}
val productsTable = TableQuery[Products]
```

As you can see above we're using simple data types (not the refined ones) to have a more easy Slick implementation. However we can also use refined types for the price of using either the slick-refined library or writing custom column mappers.
Next we'll implement the table for the translations which will also need some constraints.

{caption: "Slick translations table definition"}
```scala
final class Names(tag: Tag) extends Table[(UUID, String, String)](tag, "names") {
  def productId = column[UUID]("product_id")
  def langCode  = column[String]("lang_code")
  def name      = column[String]("name")

  def pk = primaryKey("names_pk", (productId, langCode))
  def productFk =
    foreignKey("names_product_id_fk", productId, productsTable)(
      _.id,
      onDelete = ForeignKeyAction.Cascade,
      onUpdate = ForeignKeyAction.Cascade
    )

  def * = (productId, langCode, name)
}
val namesTable = TableQuery[Names]
```

As you can see the definition of constraints is also pretty simple. Now our repository needs some functions for a more convenient access to the data.

{caption: "Slick repository functions"}
```scala
def loadProduct(id: ProductId): Future[Seq[(UUID, String, String)]] = {
  val program = for {
    (p, ns) <- productsTable
      .filter(_.id === id)
      .join(namesTable)
      .on(_.id === _.productId)
  } yield (p.id, ns.langCode, ns.name)
  dbConfig.db.run(program.result)
}

def loadProducts(): DatabasePublisher[(UUID, String, String)] = {
  val program = for {
    (p, ns) <- productsTable.join(namesTable)
                 .on(_.id === _.productId).sortBy(_._1.id)
  } yield (p.id, ns.langCode, ns.name)
  dbConfig.db.stream(program.result)
}

def saveProduct(p: Product): Future[List[Int]] = {
  val cp      = productsTable += (p.id)
  val program = DBIO.sequence(
    cp :: saveTranslations(p).toList
  ).transactionally
  dbConfig.db.run(program)
}

def updateProduct(p: Product): Future[List[Int]] = {
  val program = namesTable
    .filter(_.productId === p.id)
    .delete
    .andThen(DBIO.sequence(saveTranslations(p).toList))
    .transactionally
  dbConfig.db.run(program)
}

protected def saveTranslations(p: Product): NonEmptyList[DBIO[Int]] = {
  val save = saveTranslation(p.id)(_)
  p.names.toNonEmptyList.map(t => save(t))
}

/**
  * Create a query to insert or update a given translation in the database.
  *
  * @param id The unique ID of the product.
  * @param t  The translation to be saved.
  * @return A composable sql query for Slick.
  */
protected def saveTranslation(id: ProductId)(t: Translation): DBIO[Int] =
  namesTable.insertOrUpdate((id, t.lang, t.name))
```

The last two functions are helpers to enable us to create a load queries which we can compose. They are used in the `saveProduct` and `updateProduct` functions to create a list of queries that are executed as bulk while the call to `transactionally` ensures that they will run within a transaction. When updating a product we first delete all existing translations to allow the removal of existing translations via an update. To be able to do so we use the `andThen` helper from Slick.
The `loadProduct` function simply returns a list of database rows from the needed join. Therefore we need a function which builds a `Product` type out of that.

{caption: "Helper function to create a Product"}
```scala
def fromDatabase(rows: Seq[(UUID, String, String)]): Option[Product] = {
  val po = for {
    (id, c, n) <- rows.headOption
    t          <- Translation.fromUnsafe(c)(n)
    p          <- Product(
                    id = id,
                    names = NonEmptySet.one[Translation](t)
                  ).some
  } yield p
  po.map(
    p =>
      rows.drop(1).foldLeft(p) { (a, cols) =>
        val (id, c, n) = cols
        Translation.fromUnsafe(c)(n).fold(a)(t =>
          a.copy(names = a.names.add(t))
        )
    }
  )
}
```

But oh no! The compiler refuses to build it:

{caption: "Missing cats.Order"}
```text
[error] .../impure/models/Product.scala:45:74:
  could not find implicit value for parameter A: 
    cats.kernel.Order[com.wegtam.books.pfhais.impure.models.Translation]
[error] p <- Product(id = id, names = NonEmptySet.one[Translation](t)).some
[error]                                                            ^
```

It seems we have to provide an instance of `Order` for our `Translation` model to make Cats happy. So we have think of an ordering for our model. A simple approach would be to simply order by the language code. Let's try this:

{caption: "Providing Order for LanguageCode"}
```scala
import cats._
import cats.syntax.order._

implicit val orderLanguageCode: Order[LanguageCode] = 
  new Order[LanguageCode] {
    def compare(x: LanguageCode, y: LanguageCode): Int =
      x.value.compare(y.value)
  }
```

I> Please note that you will need to import either the `syntax.order._` package or the `implicits` package from Cats.

You might have noticed the explicit call to `.value` to get the underlying string instance of our refined type. This is needed because the other option (using `x.compare(y)`) will compile but bless you with stack overflow errors. The reason is probably that the latter is compiled into code calling `OrderOps#compare` which is recursive.

{caption: "Providing Order for Translation"}
```scala
import cats._
import cats.syntax.order._

implicit val order: Order[Translation] =
  new Order[Translation] {
    def compare(x: Translation, y: Translation): Int =
      x.lang.compare(y.lang)
  }
```

So far we should have everything in place to make use of our database. Now we need to wire it all together.

## Akka-HTTP routes

Defining the routes is pretty simple if you're used to the Akka-HTTP routing DSL syntax.

{caption: "Basic routes with Akka-HTTP"}
```scala
val route = path("product" / ProductIdSegment) { id: ProductId =>
  get {
    ???
  } ~ put {
    ???
  }
} ~ path("products") {
  get {
    ???
  } ~
  post {
    ???
  }
}
```

We will fill in the details later on. But now for starting the actual server to make use of our routes.

{caption: "Starting an Akka-HTTP server"}
```scala
val host       = system.settings.config.getString("api.host")
val port       = system.settings.config.getInt("api.port")
val srv        = Http().bindAndHandle(route, host, port)
val pressEnter = StdIn.readLine()
srv.flatMap(_.unbind()).onComplete(_ => system.terminate())
```

The code will fire up a server using the defined routes and hostname and port from the configuration to start a server. It will run until you press enter and then terminate. Let us now visit the code for each routing endpoint. We will start with the one for returning a single product.

{caption: "Returning a single product"}
```scala
path("product" / ProductIdSegment) { id: ProductId =>
  get {
    complete {
      for {
        rows <- repo.loadProduct(id)
        prod <- Future { Product.fromDatabase(rows) }
      } yield prod
    }
  }
}
```

We load the raw product data from the repository and convert it into a proper product model. But to make the types align we have to wrap the second call in a `Future` otherwise we would get a compiler error. We don't need to marshal the response because we are using the akka-http-json library which provides for example an `ErrorAccumulatingCirceSupport` import that handles this. Unless of course you do not have circe codecs defined for your types.

{caption: "Updating a single product"}
```scala
val route = path("product" / ProductIdSegment) { id: ProductId =>
  put {
    entity(as[Product]) { p =>
      complete {
        repo.updateProduct(p)
      }
    }
  }
}
```

The route for updating a product is also very simple. We're extracting the product entity via the `entity(as[T])` directive from the request body and simply give it to the appropriate repository function. Now onwards to creating a new product.

{caption: "Creating a product"}
```scala
path("products") {
  post {
    entity(as[Product]) { p =>
      complete {
        repo.saveProduct(p)
      }
    }
  }
}
```

As you can see the function is basically the same except that we're calling a different function from the repository. Last but not least let us take a look at the return all products endpoint.

{caption: "Return all products"}
```scala
path("products") {
  get {
    complete {
      val products = for {
        rows <- repo.loadProducts()
        ps <- Future {
          rows.toList.groupBy(_._1).map {
            case (_, cols) => Product.fromDatabase(cols)
          }
        }
      } yield ps
      products.map(_.toList.flatten)
    }
  }
}
```

This looks more complicated that the other endpoints. So what exactly are we doing here?
Well first we load the raw product data from the repository. Afterwards we convert it into the proper data model or to be more exact into a list of product entities.

Q> What is wrong with that approach?

The first thing that comes to mind is that we're performing operations in memory. This is not different from the last time when we converted the data for a single product. Now however we're talking about all products which may be a lot of data. Another obvious point is that we get a list of `Option[Product]` which we explicitly flatten at the end.

Q> So, how can we do better?

Maybe we should consider streaming the results. But we still have to group and combine the rows which belong to a single product into a product entity. Can we achieve that with streaming? Well, let's look at our data flow.
We receive a list of 3 columns from the database in the following format: `product id, language code, name`. The tricky part being that multiple rows (list entries) can belong to the same product recognizable by the same value for the first column `product id`. At first we should simplify our problem by ensuring that the list will be sorted by the `product id`. This is done by adjusting the function `loadProducts` in the repository.

{caption: "Sort the returned list of entries."}
```scala
def loadProducts(): DatabasePublisher[(UUID, String, String)] = {
  val program = for {
    (p, ns) <- productsTable.join(namesTable).on(_.id === _.productId)
               .sortBy(_._1.id)
  } yield (p.id, ns.langCode, ns.name)
  dbConfig.db.stream(program.result)
}
```

Now we can rely on the fact that we have seen all entries for one product if the product id in our list changes. Let's adjust our code in the endpoint to make use of streaming now. Because Akka-HTTP is based on Akka-Streams we can simply use that.

{caption: "Return all products as stream"}
```scala
path("products") {
  get {
    implicit val jsonStreamingSupport: JsonEntityStreamingSupport =
      EntityStreamingSupport.json()

    val src = Source.fromPublisher(repo.loadProducts())
    val products: Source[Product, NotUsed] = src
      .collect(
        cs =>
          Product.fromDatabase(Seq(cs)) match {
            case Some(p) => p
        }
      )
      .groupBy(Int.MaxValue, _.id)
      .fold(Option.empty[Product])(
        (op, x) => op.fold(x.some)(p =>
          p.copy(names = p.names ::: x.names).some
        )
      )
      .mergeSubstreams
      .collect(
        op =>
          op match {
            case Some(p) => p
        }
      )
    complete(products)
  }
}
```

Wow, this may look scary but let's break it apart piece by piece. At first we need an implicit value which provides streaming support for JSON. Next we create a `Source` from the database stream. Now we implement the processing logic via the high level streams API. We collect every defined output of our helper function `fromDatabase` which leads to a stream of `Product` entities. But we have created way too many (Each product will be created as often as it has translations.). So we group our stream by the product id which creates a new stream for each product id holding only the entities for the specific product. We fold over each of these streams by merging together the list of translations (`names`). Afterwards we merge the streams back together and run another collect function to simply get a result stream of `Product` and not of `Option[Product]`. Last but not least the stream is passed to the `complete` function which will do the right thing.

### Problems with the solution

The solution has two problems:

1. The number of individual streams (and thus products) is limited to `Int.MaxValue`.
2. The `groupBy` operator holds the references to these streams in memory opening a possible out of memory issue here.

As the first problem is simply related to the usage of `groupBy` we may say that we only have one problem: The usage of `groupBy`. ;-)
For a limited amount of data the proposed solution is perfectly fine so we will leave it as is for now.

Regarding the state of our service we have a working solution, so congratulations and let's move on to the pure implementation.

# Pure implementation

Like in the previous section I will spare you the details of the sbt setup. We will be using the following set of libraries:

1. http4s
2. Doobie (as database layer)
3. Flyway for database migrations (or evolutions)
4. Circe for JSON codecs
5. Refined for using refined types
6. the PostgreSQL JDBC driver
7. pureconfig (for proper configuration loading)

## Pure configuration handling

Last time we simply loaded our configuration via the typesafe config library but can't we do a bit better here? The answer is yes by using the pureconfig[^12] library. First we start by implementing the necessary parts of our configuration as data types.

{caption: "Configuration data types"}
```scala
final case class ApiConfig(host: NonEmptyString, port: PortNumber)

object ApiConfig {
  implicit val configReader: ConfigReader[ApiConfig] =
    deriveReader[ApiConfig]
}

final case class DatabaseConfig(driver: NonEmptyString,
                                url: DatabaseUrl,
                                user: DatabaseLogin,
                                pass: DatabasePassword)

object DatabaseConfig {
  implicit val configReader: ConfigReader[DatabaseConfig] =
    deriveReader[DatabaseConfig]
}
```

As we can see the code is pretty simple. The implicits in the companion objects are needed for pureconfig to actually map from a configuration to your data types. As you can see we are using a function `deriveReader` which will derive (like in mathematics) the codec (Yes, it is similar to a JSON codec thus the name.) for us.

I> **A note on derivation**
I> In general we always want the compiler to derive stuff automatically because it means less work for us. However...
I> As always there is a cost and sometimes a rather big one (compile time). Therefore you should not use fully automatic derivation but the semi automatic variant instead. The latter will let you chose what to derive explicitly.
I> In some circumstances it may even be better to generate a codec manually (complex, deeply nested models).

## Models

Because we have already written our models we just re-use them here. The only thing we change is the semi automatic derivation of the JSON codecs. We just need to import the appropriate circe package and call the derive functions.

{caption: "Derive JSON codecs"}
```scala
import io.circe._
import io.circe.generic.semiauto._

implicit val decode: Decoder[Product] = deriveDecoder[Product]
implicit val encode: Encoder[Product] = deriveEncoder[Product]
implicit val decode: Decoder[Translation] = deriveDecoder[Translation]
implicit val encode: Encoder[Translation] = deriveEncoder[Translation]
```

I> Remind please that for complex classes this can add significant compile time overhead.
I> Also using (semi) automatic derivation for data models that are used in a public API may result in breaking changes if an attribute name is changed.

## Database layer

In general the same applies to the database layer as we have already read in the "impure" section.

### Migrations

For the sake of simplicity we will stick to Flyway for our database migrations. However we will wrap the migration code in a different way (read *Encapsulate it properly within an `IO` to defer side effects.*). While we're at it we may just as well write our migration code using the interpreter pattern (it became famous under the name "tagless final" in Scala).

{caption: "Database migrator base"}
```scala
trait DatabaseMigrator[F[_]] {
  def migrate(url: DatabaseUrl,
              user: DatabaseLogin,
              pass: DatabasePassword): F[Int]
}
```

We define a trait which describes the functionality desired by our interpreter and use a higher kinded type parameter to be able to abstract over the type. But now let's continue with our Flyway interpreter.

{caption: "Flyway migrator interpreter"}
```scala
final class FlywayDatabaseMigrator extends DatabaseMigrator[IO] {
  override def migrate(url: DatabaseUrl,
                       user: DatabaseLogin,
                       pass: DatabasePassword): IO[Int] =
    IO {
      val flyway: Flyway = Flyway.configure()
        .dataSource(url, user, pass)
        .load()
      flyway.migrate()
    }
}
```

As we can see, the implementation is pretty simple and we just wrap our code into an `IO` monad to constrain the effect. Having the migration code settled we can move on to the repository.

Q> So what is wrong with our solution?

If we take a closer look at the method definition of `Flyway.migrate`, we see this:

{caption: "Method definition of Flyway.migrate"}
```scala
public int migrate() throws FlywayException
```

While `IO` will gladly defer side effects for us it won't stop enclosed code from throwing exceptions. This is not that great. So what can we do about it?
Having an instance of `MonadError` in scope we could just use the `.attempt` function provided by it. But is this enough or better does this provide a sensible solution for us? Let's play a bit on the REPL.

{caption: "MonadError on the REPL"}
```scala
@ import cats._, cats.effect._, cats.implicits._
@ val program = for {
           _ <- IO(println("one"))
           _ <- IO(println("two"))
           x <- IO.pure(42)
           } yield x
@ program.attempt.unsafeRunSync match {
           case Left(e) =>
             println(e.getMessage)
             -1
           case Right(r) => r
           }
one
two
res3: Int = 42
@ val program = for {
           _ <- IO(println("one"))
           _ <- IO(throw new Error("BOOM!"))
           x <- IO.pure(42)
           } yield x
@ program.attempt.unsafeRunSync match {
           case Left(e) =>
             println(e.getMessage)
             -1
           case Right(r) => r
           }
one
BOOM!
res5: Int = -1
```

This looks like we just have to use `MonadError` then. Hurray, we don't need to change our code in the migrator. As model citizens of the functional programming camp we just defer the responsibility upwards to the calling site.

### Doobie

As we already started with using a tagless final approach we might as well continue with it and define a base for our repository.

{caption: "Base trait for the repository"}
```scala
trait Repository[F[_]] {
  def loadProduct(id: ProductId): F[Seq[(ProductId, LanguageCode, ProductName)]]

  def loadProducts(): Stream[F, (ProductId, LanguageCode, ProductName)]

  def saveProduct(p: Product): F[Int]

  def updateProduct(p: Product): F[Int]
}
```

There is nothing exciting here except that we feel brave now and try to use proper refined types in our database functions. This is possible due to the usage of the doobie-refined module. To be able to map the `UUID` data type (and others) we also need to include the doobie-postgresql module. For convenience we are still using `ProductId` instead of `UUID` in our definition. In addition we wire the return type of `loadProducts` to be a `fs2.Stream` because we want to achieve pure functional streaming here. :-)
So let's see what a repository using doobie looks like.

{caption: "The doobie repository."}
```scala
final class DoobieRepository[F[_]: Sync](tx: Transactor[F])
  extends Repository[F] {

  override def loadProduct(id: ProductId) = ???

  override def loadProducts() = ???

  override def saveProduct(p: Product) = ???

  override def updateProduct(p: Product) = ???
}
```

We keep our higher kinded type as abstract as we can but we want it to be able to suspend our side effects. Therefore we require an implicit `Sync`.[^13]
If we look at the detailed function definitions further below, the first big difference is that with doobie you write plain SQL queries. You can do this with Slick too[^14] but with doobie it is the only way. If you're used to object relational mapping (ORM) or other forms of query compilers then this may seem strange at first. But: "In data processing it seems, all roads eventually lead back to SQL!"[^15] ;-)
We won't discuss the benefits or drawbacks here but in general I also lean towards the approach of using the de facto lingua franca for database access because it was made for this and so far no query compiler was able to beat hand crafted SQL in terms of performance. Another benefit is that if you ask a database guru for help, she will be much more able to help you with plain SQL queries than with some meta query which is compiled into something that you have no idea of.

{caption: "Loading a product."}
```scala
override def loadProduct(id: ProductId) = 
  sql"""SELECT products.id, names.lang_code, names.name 
        FROM products
        JOIN names ON products.id = names.product_id
        WHERE products.id = $id"""
    .query[(ProductId, LanguageCode, ProductName)]
    .to[Seq]
    .transact(tx)
```

The `loadProduct` function simply returns all rows for a single product from the database like its Slick counterpart in the impure variant. The parameter will be correctly interpolated by Doobie therefore we don't need to worry about SQL injections here. We specify the type of the query, instruct Doobie to transform it into a sequence and give it to the transactor.

I> Please note that instead of the Slick variant the code does not run at that point!
I> While the `db.run` of Slick will run your code the `transact` of Doobie will not. It just provides a free structure (read *free monads*) which can be interpreted later on.

{caption: "Load all products"}
```scala
override def loadProducts() =
  sql"""SELECT products.id, names.lang_code, names.name
      FROM products
      JOIN names ON products.id = names.product_id
      ORDER BY products.id"""
    .query[(ProductId, LanguageCode, ProductName)]
    .stream
    .transact(tx)
```

Our `loadProducts` function is equivalent to the first one but it returns the data for all products sorted by product and as a stream using the fs2 library which provides pure functional streaming.

{caption: "Save a product"}
```scala
override def saveProduct(p: Product): F[Int] = {
  val namesSql = 
    "INSERT INTO names (product_id, lang_code, name) VALUES (?, ?, ?)"
  val namesValues = p.names.map(t => (p.id, t.lang, t.name))
  val program = for {
    pi <- sql"INSERT INTO products (id) VALUES(${p.id})".update.run
    ni <- Update[(ProductId, LanguageCode, ProductName)](namesSql)
            .updateMany(namesValues)
  } yield pi + ni
  program.transact(tx)
}
```

When saving a product we use monadic notation for our program to have it short circuit in the case of failure. Doobie will also put all commands into a database transaction. The function itself will try to create the "master" entry into the products table and save all translations afterwards.

{caption: "Update a product"}
```scala
override def updateProduct(p: Product): F[Int] = {
  val namesSql =
    "INSERT INTO names (product_id, lang_code, name) VALUES (?, ?, ?)"
  val namesValues = p.names.map(t => (p.id, t.lang, t.name))
  val program = for {
    dl <- sql"DELETE FROM names WHERE product_id = ${p.id}".update.run
    ts <- Update[(ProductId, LanguageCode, ProductName)](namesSql)
            .updateMany(namesValues)
  } yield dl + ts
  program.transact(tx)
}
```

The `updateProduct` function uses also monadic notation like the `saveProduct` function we talked about before. The difference is that it first deletes all known translations before saving the given ones.

## http4s routes

The routing DSL of http4s differs from the one of Akka-HTTP. Although I like the latter one more it poses no problem to model out a base for our routes.

{caption: "Base for http4s routes"}
```scala
val productRoutes: HttpRoutes[IO] = HttpRoutes.of[IO] {
  case GET -> Root / "product" / id =>
    ???
  case PUT -> Root / "product" / id =>
    ???
}
val productsRoutes: HttpRoutes[IO] = HttpRoutes.of[IO] {
  case GET -> Root / "products" =>
    ???
  case POST -> Root / "products" =>
    ???
}
```

As we can see the DSL is closer to Scala syntax and quite easy to read. But before we move on to the details of each route let's think about how we can model this a bit more abstract. While it is fine to have our routes bound to `IO` it would be better to have more flexibility here. We have several options here but for starters we just extract our routes into their own classes like in the following schema.

{caption: "Routing classes"}
```scala
final class ProductRoutes[F[_]: Sync](repo: Repository[F])
  extends Http4sDsl[F] {

  val routes: HttpRoutes[F] = HttpRoutes.of[F] {
    case GET -> Root / "product" / UUIDVar(id) =>
      ???
    case req @ PUT -> Root / "product" / UUIDVar(id) =>
      ???
  }
}

final class ProductsRoutes[F[_]: Sync](repo: Repository[F]) 
  extends Http4sDsl[F] {

  val routes: HttpRoutes[F] = HttpRoutes.of[F] {
    case GET -> Root / "products" =>
      ???
    case req @ POST -> Root / "products" =>
      ???
  }
}
```

So far they only need the repository to access and manipulate data. Now let's take on the single route implementations.

{caption: "Product routes"}
```scala
final class ProductRoutes[F[_]: Sync](repo: Repository[F])
  extends Http4sDsl[F] {
  implicit def decodeProduct = jsonOf
  implicit def encodeProduct = jsonEncoderOf

  val routes: HttpRoutes[F] = HttpRoutes.of[F] {
    case GET -> Root / "product" / UUIDVar(id) =>
      for {
        rows <- repo.loadProduct(id)
        resp <- Ok(Product.fromDatabase(rows))
      } yield resp
    case req @ PUT -> Root / "product" / UUIDVar(id) =>
      for {
        p <- req.as[Product]
        _ <- repo.updateProduct(p)
        r <- NoContent()
      } yield r
  }
}
```

First we need to bring JSON codecs in scope for http4s thus the implicit definitions on top of the file. In the route for loading a single product we simply load the database rows which we pipe through our helper function to construct a proper `Product` and return that.
The update route (via `PUT`) transforms the request body into a `Product` and gives that to the update function of the repository. Finally a `NoContent` response is returned.

{caption: "Products routes (1st try)"}
```scala
final class ProductsRoutes[F[_]: Sync](repo: Repository[F])
  extends Http4sDsl[F] {
  implicit def decodeProduct = jsonOf
  implicit def encodeProduct = jsonEncoderOf

  val routes: HttpRoutes[F] = HttpRoutes.of[F] {
    case GET -> Root / "products" =>
      val ps: Stream[F, Product] = repo.loadProducts
        .map(cs => Product.fromDatabase(List(cs)))
        .collect {
          case Some(p) => p
        }
      Ok(ps)
    case req @ POST -> Root / "products" =>
      for {
        p <- req.as[Product]
        _ <- repo.saveProduct(p)
        r <- NoContent()
      } yield r
  }
}
```

Our first take on the routes for products looks pretty complete already. Again we need implicit definitions for our JSON codecs to be able to serialize and de-serialize our entities. The `POST` route for creating a product is basically the same as the update route from the previous part. We create a `Product` from the request body, pass it to the save function of the repository and return a 205 `NoContent` response.
The `GET` route for returning all products calls the appropriate repository function which returns a stream which we map over using our helper function. Afterwards we use `collect` to convert our stream from `Option[Product]` to a stream of `Product` which we pass to the `Ok` function of http4s.

I> The attentive reader will have noticed that our "fetch all products" implementation faces the same problem as our first try in the impure section: We return to many products!

To solve this we need to dive into the fs2 API and leverage it's power to merge our products back together. So let's see how we do.

### Streaming - Take 1

Because we believe ourselves to be clever we pick the simple sledge hammer approach and just run some accumulator on the stream. So what do we need? A helper function and some code changes on the stream (e.g. in the route).

{caption: "Merging products"}
```scala
def merge(ps: List[Product])(p: Product): List[Product] =
  ps.headOption.fold(List(p)) { h =>
    if (h.id === p.id)
      h.copy(names = h.names ::: p.names) :: ps.drop(1)
    else
      p :: ps
  }
```

So this function will take a list (that may be empty) and a product and will merge the top most element (the head) of the list with the given one. It will return an updated list that either contains an updated head element or a new head. Leaving aside the question of who guarantees that the relevant list element will always be the head, we may use it.

{caption: "Adapted streaming route"}
```scala
case GET -> Root / "products" =>
  val ps: Stream[F, Product] = repo.loadProducts
    .map(cs => Product.fromDatabase(List(cs)))
    .collect {
      case Some(p) => p
    }
    .fold(List.empty[Product])((acc, p) => Product.merge(acc)(p))
  Ok(ps)
```

Looks so simple, does it? Just a simple `fold` which uses our accumulator and we should be settled. But life is not that simple...

{caption: "Compiler error"}
```text
found   : fs2.Stream[F,List[com.wegtam.books.pfhais.pure.models.Product]]
required: fs2.Stream[F,com.wegtam.books.pfhais.pure.models.Product]
         .fold(List.empty[Product])((acc, p) => Product.merge(acc)(p))
              ^
```

The compiler complains that we have changed the type of the stream and rightly so. So let's fix that compiler error.

Q> Should we really do this?

Let's take a look again and think about what it means to change a stream of products into a stream of a list of products. It means that we will be building the whole thing in memory! Well if we wanted that we could have skipped streaming at all. So back to the drawing board.

### Streaming - Take 2

We need to process our stream of database columns (or products if we use the converter like before) in such a way that all related entities will be grouped into one product and emitted as such. After browsing the documentation of fs2 we stumble upon a function called `groupAdjacentBy` so we try that one.

{caption: "Proper streaming of products"}
```scala
case GET -> Root / "products" =>
  val ps = repo.loadProducts
    .groupAdjacentBy(_._1)
    .map {
      case (id, rows) => Product.fromDatabase(rows.toList)
    }
    .collect {
      case Some(p) => p
    }
  Ok(ps)
```

Okay, this does not look complicated and it even compiles - Hooray! :-)
So let's break it apart piece by piece. The group function of fs2 will partition the input depending on the given function into chunks. A `Chunk` is used internally by fs2 for all kinds of stuff. You may compare it to a sub-stream of Akka-Streams. However the documentation labels it as: *Strict, finite sequence of values that allows index-based random access of elements.*
Having our chunks we can map over each one converting it into a list which is then passed to our helper function `fromDatabase` to create proper products. Last but not least we need to collect our entities to get from an `Option[Product]` to a stream of `Product`.

### JSON trouble

Now that we have a proper streaming solution we try it out but what do we get when we expect a list of products?

I> In case you wonder what the `http` command is, we use the tool httpie[^16] for querying our service.

{caption: "Broken JSON"}
```text
% http :53248/products
HTTP/1.1 200 OK
Content-Type: application/json
Transfer-Encoding: chunked

{
  "id":"8773899b-fcfa-401f-af3e-b188ebb0c00c",
  "names":[
    {"lang":"de","name":"Erdbeere"},
    {"lang":"en","name":"Strawberry"}
  ]
}
{
  "id":"983aaf86-abe4-44af-9896-d8f2d2c5f82c",
  "names":[
    {"lang":"de","name":"Gurke"},
    {"lang":"en","name":"Cucumber"}
  ]
}
```

Well, whatever this is, it is not JSON! It might look like it, but it isn't. However quite often you can see such things in the wild (read *in production*).

Q> So, how can we improve this?

If we think about it then this sounds like a bug in http4s and indeed we find an issue[^17] for it. Because the underlying problem is not as trivial as it first sounds maybe we should try to work around the issue.
The fs2 API offers concatenation of streams and the nifty `intersperse` function to insert elements between emitted ones. So let's give it a try.

{caption: "Fix JSON encoding issues"}
```scala
case GET -> Root / "products" =>
  val prefix = Stream.eval("[".pure[F])
  val suffix = Stream.eval("]".pure[F])
  val ps = repo.loadProducts
    .groupAdjacentBy(_._1)
    .map {
      case (id, rows) => Product.fromDatabase(rows.toList)
    }
    .collect {
      case Some(p) => p
    }
    .map(_.asJson.noSpaces)
    .intersperse(",")
  @SuppressWarnings(Array("org.wartremover.warts.Any"))
  val result: Stream[F, String] = prefix ++ ps ++ suffix
  Ok(result)
```

First we create streams for the first and last JSON that we need to emit. Please not that we cannot simply use a `String` here but have to lift it into our HKT `F`. The usage of `pure` is okay because we simply lift a fixed value. Then we extend our original stream processing by explicitly converting our products to JSON and inserting the delimiter (a comma) manually using the `intersperse` function. In the end we simply concatenate our streams and return the result.
Our solution is quite simple, having the downside that we need to suppress a warning from the wartremover[^18] tool. This is somewhat annoying but can happen. If we remove the annotation, we'll get a compiler error:

{caption: "Wartremover error"}
```text
[error] ... [wartremover:Any] Inferred type containing Any
[error] val result: Stream[F, String] = prefix ++ ps ++ suffix
[error]                                        ^
[error] ... [wartremover:Any] Inferred type containing Any
[error] val result: Stream[F, String] = prefix ++ ps ++ suffix
[error]                                              ^
[error] two errors found
```

So let's check if we have succeeded:

{caption: "Checking our JSON"}
```text
% http :53248/products
HTTP/1.1 200 OK
Content-Type: text/plain; charset=UTF-8
Transfer-Encoding: chunked

[
    {
        "id": "8773899b-fcfa-401f-af3e-b188ebb0c00c", 
        "names": [
            {
                "lang": "de",
                "name": "Erdbeere"
            },
            {
                "lang": "en",
                "name": "Strawberry"
            }
        ]
    },
    {
        "id": "983aaf86-abe4-44af-9896-d8f2d2c5f82c", 
        "names": [
            {
                "lang": "de",
                "name": "Gurke"
            },
            {
                "lang": "en",
                "name": "Cucumber"
            }
        ]
    }
]
```

This looks good, so we congratulations: We are done with our routes!

### Starting the application

Within our main entry point we simply initialise all needed components and wire them together. We'll step through each part in this section. The first thing you'll notice is that we use the `IOApp` provided by the Cats effect library[^19].

{caption: "Main application"}
```scala
object Pure extends IOApp {
  @SuppressWarnings(Array("org.wartremover.warts.Any"))
  def run(args: List[String]): IO[ExitCode] = ???
}
```

Yet again we need to suppress a warning from wartremover here. But let's continue to initialising the database connection.

{caption: "Database initialisation"}
```scala
val migrator: DatabaseMigrator[IO] = new FlywayDatabaseMigrator

val program = for {
  (apiConfig, dbConfig) <- IO {
    val cfg = ConfigFactory.load
    (loadConfigOrThrow[ApiConfig](cfg, "api"),
     loadConfigOrThrow[DatabaseConfig](cfg, "database"))
  }
  ms <- migrator.migrate(dbConfig.url, dbConfig.user, dbConfig.pass)
  tx = Transactor
    .fromDriverManager[IO](dbConfig.driver,
      dbConfig.url,
      dbConfig.user,
      dbConfig.pass)
  repo = new DoobieRepository(tx)
```

We create our database migrator explicitly wired to the `IO` data type. Now we start with a for comprehension in which we load our configuration via pureconfig yet again within an `IO`. After successful loading of the configuration we continue with migrating the database. Finally we create the transactor needed by Doobie and the database repository.

{caption: "Routes and http4s server"}
```scala
val program = for {
  // ...
  productRoutes  = new ProductRoutes(repo)
  productsRoutes = new ProductsRoutes(repo)
  routes         = productRoutes.routes <+> productsRoutes.routes
  httpApp        = Router("/" -> routes).orNotFound
  server         = BlazeServerBuilder[IO].bindHttp(apiConfig.port,
                     apiConfig.host).withHttpApp(httpApp)
  fiber          = server.resource.use(_ => IO(StdIn.readLine())).as(ExitCode.Success)
} yield fiber
```

Here we create our routes via the classes, combine them (via `<+>` operator) and create the http4s app explicitly using an `IO` thus wiring our abstract routes to `IO`. The service will - like the impure one - run until you press enter. But it won't run yet. ;-)

{caption: "Running the service"}
```scala
program.attempt.unsafeRunSync match {
  case Left(e) =>
    IO {
      println("*** An error occured! ***")
      if (e != null) {
        println(e.getMessage)
      }
      ExitCode.Error
    }
  case Right(r) => r
}
```

If you remember playing around with `MonadError` then you'll recognize the `attempt` here. We attempt to run our program and execute possible side effects via the `unsafeRunSync` method from Cats effect. But to provide a proper return type for the `IOApp` we need to evaluate the return value which is either an error or a proper exit code. In case of an error we print it out on the console (no fancy logging here) and explicitly set an error code as the return value.

I> Please note that we also wrap our error handler into an `IO` to defer possible side effects.

As it seems we are done with our pure service! Or are we? Let's see what we need add to both services if we want to test them.

# What about tests?

In the domain of strong static typing (not necessarily functional) you might hear phrases like "It compiles therefore it must be correct thus we don't need tests!". While there is a point that certain kinds of tests can be omitted in favour of strong static typing such stances overlook that even a correctly typed program may produce the wrong output. The other extreme (coming from dynamic typed land) is to substitute typing with testing - which is even worse. Remember that testing is usually a probabilistic approach and cannot guarantee the absence of bugs. If you have ever refactored a large code base in both paradigms then you'll very likely come to esteem a good type system.

However, we need tests, so let's write some. But before let us think a bit about what kinds of tests we need. :-)
Our service must read and create data in the JSON format. This format should be fixed and changes to it should raise some red flag because: Hey, we just broke our API! Furthermore we want to unleash the power of ScalaCheck[^10] to benefit from property based testing. But even when we're not using that we can still use it to generate test data for us.
Besides the regular unit tests there should be integration tests if a service is written. We can test a lot of things on the unit test side but in the end the integration of all our moving parts is what matters and often (usually on the not so pure side of things) you would have to trick (read *mock*) a lot to test things in isolation.

## Testing the impure service

We will start with writing some data generators using the ScalaCheck library.

### Generators

ScalaCheck already provides several generators for primitives but for our data models we have to do some more plumbing. Let's start with generating a language code.

{caption: "ScalaCheck generate LanguageCode"}
```scala
val genLanguageCode: Gen[LanguageCode] = Gen.oneOf(LanguageCodes.all)
```

Using the `Gen.oneOf` helper from the library the code becomes dead simple. Generating a `UUID` is nothing special either.

{caption: "ScalaCheck generate UUID"}
```scala
val genUuid: Gen[UUID] = Gen.delay(UUID.randomUUID)
)
```

You might be tempted to use `Gen.const` here but please don't because that one will be memorized and thus never change. Another option is using a list of randomly generated UUID values from which we then chose one. That would be sufficient for generators which only generate a single product but if we want to generate lists of them we would have duplicate ids sooner than later.

{caption: "ScalaCheck generate ProductName"}
```scala
val DefaultProductName: ProductName = "I am a product name!"
val genProductName: Gen[ProductName] = for {
  cs <- Gen.nonEmptyListOf(Gen.alphaNumChar)
  name = RefType.applyRef[ProductName](cs.mkString)
           .getOrElse(DefaultProductName)
} yield name
```

So what do we have here? We want to generate a non empty string (because that is a requirement for our `ProductName`) but we also want to return a properly typed entity. First we let ScalaCheck generate a non empty list of random characters which we give to a utility function of refined. However we need a fallback value in case the validation done by refined fails. Therefore we defined a general default product name beforehand.

I> If the data generated will always be valid then we can simplify the code above by using the `Refined.unsafeApply` function. This reduces code complexity but may introduce bugs in your tests which are hard to track down.

{caption: "ScalaCheck using Refined unsafeApply"}
```scala
val genProductName: Gen[ProductName] =
  Gen.nonEmptyListOf(Gen.alphaNumChar).map(cs => 
    Refined.unsafeApply(cs.mkString)
  )
```

Now that we have generators for language codes and product names we can write a generator for our `Translation` type.

{caption: "ScalaCheck generate Translation"}
```scala
val genTranslation: Gen[Translation] = for {
  c <- genLanguageCode
  n <- genProductName
} yield
  Translation(
    lang = c,
    name = n
  )

implicit val arbitraryTranslation: Arbitrary[Translation] =
  Arbitrary(genTranslation)
```

As we can see the code is also quite simple. Additionally we create an implicit arbitrary value which will be used automatically by the `forAll` test helper if it is in scope. To be able to generate a `Product` we will need to provide a non empty list of translations.

{caption: "ScalaCheck generate lists of translations"}
```scala
val genTranslationList: Gen[List[Translation]] = for {
  ts <- Gen.nonEmptyListOf(genTranslation)
} yield ts

val genNonEmptyTranslationList: Gen[NonEmptyList[Translation]] = for {
  t  <- genTranslation
  ts <- genTranslationList
  ns = NonEmptyList.fromList(ts)
} yield ns.getOrElse(NonEmptyList.of(t))
```

The first generator will create a non empty list of translations but will be typed as a simple `List`. Therefore we create a second generator which uses the `fromList` helper of the non empty list from Cats. Because that helper returns an `Option` (read *is a safe function*) we need to fallback to using a simple `of` function at the end.
With all these in place we can finally create our `Product` instances.

{caption: "ScalaCheck generate Product"}
```scala
val genProduct: Gen[Product] = for {
  id <- genProductId
  ts <- genNonEmptyTranslationList
} yield
  Product(
    id = id,
    names = ts
  )

implicit val arbitraryProduct: Arbitrary[Product] =
  Arbitrary(genProduct)
```

The code is basically the same as for `Translation` - the arbitrary implicit included.

### Unit Tests

To avoid repeating the construction of our unit test classes we will implement a base class for tests which is quite simple.

{caption: "Base class for unit tests"}
```scala
abstract class BaseSpec extends WordSpec 
  with MustMatchers with ScalaCheckPropertyChecks {}
```

Feel free to use other test styles - after all ScalaTest offers a lot[^21] of them. I tend to lean towards the more verbose ones like `WordSpec`. Maybe that is because I spent a lot of time with RSpec[^22] in the Ruby world. ;-)

{caption: "Testing Product#fromDatabase"}
```scala
import com.wegtam.books.pfhais.impure.models.TypeGenerators._

forAll("input") { p: Product =>
  val rows = p.names.map(t => (p.id, t.lang.value, t.name.value)).toList
  Product.fromDatabase(rows) must contain(p)
}
```

The code above is a very simple test of our helper function `fromDatabase` which works in the following way:

1. The `forAll` will generate a lot of `Product` entities using the generator.
2. From each entity a list of "rows" is constructed like they would appear in the database.
3. These constructed rows are given to the `fromDatabase` function.
4. The returned `Option` must then contain the generated value.

Because we construct the input for the function from a valid generated instance the function must always return a valid output.

Now let's continue with testing our JSON codec for `Product`.

#### Testing a JSON codec

We need our JSON codec to provide several guarantees:

1. It must fail to decode invalid JSON input format (read *garbage*).
2. It must fail to decode valid JSON input format with invalid data (read *wrong semantics*).
3. It must succeed to decode completely valid input.
4. It must encode JSON which contains all fields included in the model.
5. It must be able to decode JSON that itself encoded.

The first one is pretty simple and to be honest: You don't have to write a test for this because that should be guaranteed by the Circe library. Things look a bit different for very simple JSON representations though (read *when encoding to numbers or strings*).
I've seen people arguing about point 5 and there may be applications for it but implementing encoders and decoders in a non-reversible way will make your life way more complicated.

{caption: "Testing decoding garbage input"}
```scala
forAll("input") { s: String =>
  decode[Product](s).isLeft must be(true)
}
```

There is not much to say about the test above: It will generate a lot of random strings which will be passed to the decoder which must fail.

{caption: "Testing invalid input values"}
```scala
forAll("id", "names") { (id: String, ns: List[String]) =>
  val json = """{
    |"id":""" + id.asJson.noSpaces + """,
    |"names":""" + ns.asJson.noSpaces + """
    |}""".stripMargin
  decode[Product](json).isLeft must be(true)
}
```

This test will generate random instances for `id` which are all wrong because it must be a UUID and not a string. Also the instances for `names` will mostly (but maybe not always) be wrong because there might be empty strings or an even empty list. So the decoder is given a valid JSON format but invalid values, therefore it must fail.

{caption: "Testing valid input"}
```scala
forAll("input") { i: Product =>
  val json = s"""{
    |"id": ${i.id.asJson.noSpaces},
    |"names": ${i.names.asJson.noSpaces}
    |}""".stripMargin
  withClue(s"Unable to decode JSON: $json") {
    decode[Product](json) match {
      case Left(e)  => fail(e.getMessage)
      case Right(v) => v must be(i)
    }
  }
}
```

In this case we manually construct a valid JSON input using values from a generated valid `Product` entity. This is passed to the decoder and the decoder must not only succeed but return an instance equal to the generated one.

{caption: "Test included fields"}
```scala
forAll("input") { i: Product =>
  val json = i.asJson.noSpaces
  json must include(s""""id":${i.id.asJson.noSpaces}""")
  json must include(s""""names":${i.names.asJson.noSpaces}""")
}
```

The test will generate again a lot of entities and we construct a JSON string from each. We then expect the string to include several field names and their correctly encoded values. You might ask why we do not check for more things like: Are these fields the *only* ones within the JSON string? Well, this would be more cumbersome to test and a JSON containing more fields than we specify won't matter for the decoder because it will just ignore them.

I> You might be tempted to produce a JSON string manually (like in the other tests) and simply compare the generated one to it. Please beware because JSON does not provide stable sorted attributes (fields). So you might get false positives because the compared strings are semantically equal but formally different.

{caption: "Decoding encoded JSON"}
```scala
forAll("input") { p: Product =>
  decode[Product](p.asJson.noSpaces) match {
    case Left(_)  => fail("Must be able to decode encoded JSON!")
    case Right(d) => withClue("Must decode the same product!")(d must be(p))
  }
}
```

Here we encode a generated entity and pass it to the encoder which must return the same entity.

#### More tests

So this is basically what we do for models. Because we have more than one we will have to write tests for each of the others. I will spare you the JSON tests for `Translation` but that one also has a helper function called `fromUnsafe` so let's take a look at the function.

{caption: "Translation#fromUnsafe"}
```scala
def fromUnsafe(lang: String)(name: String): Option[Translation] =
  for {
    l <- RefType.applyRef[LanguageCode](lang).toOption
    n <- RefType.applyRef[ProductName](name).toOption
  } yield Translation(lang = l, name = n)
```

This function simply tries to create a valid `Translation` entity from unsafe input values using the helpers provided by refined. As we can see it is a total function (read *is safe to use*). To cover all corner cases we must test it with safe and unsafe input.

{caption: "Testing Translation#fromUnsafe (1)"}
```scala
forAll("lang", "name") { (l: String, n: String) =>
  whenever(
    RefType
      .applyRef[LanguageCode](l)
      .toOption
      .isEmpty || RefType.applyRef[ProductName](n).toOption.isEmpty
  ) {
    Translation.fromUnsafe(l)(n) must be(empty)
  }
}
```

Here we generate two random strings which we explicitly check to be invalid using the `whenever` helper. Finally the function must return an empty `Option` e.g. `None` for such values.

{caption: "Testing Translation#fromUnsafe (2)"}
```scala
forAll("input") { t: Translation =>
  Translation.fromUnsafe(t.lang.value)(t.name.value) must contain(t)
}
```

The test for valid input is very simple because we simply use the values from our automatically generated valid instances. :-)

Q> So what is left to test?

So far we have no tests for our `Repository` class which handles all the database work. Neither have we tests for our routes. We have several options for testing here but before can test either of them we have do to some refactoring. For starters we should move our routes out of our main application into separate classes to be able to test them more easily.

Q> But should we refactor code only to gain better testability?

Yes, we should. There are of course limits and pros and cons to that but in general this makes sense. Also this has nothing to do with being "impure" or "pure" but with clean structure.

#### Some refactoring

Moving the routes into separate classes poses no big problem we simply create a `ProductRoutes` and a `ProductsRoutes` class which will hold the appropriate routes. As a result our somewhat messy main application code becomes more readable.

{caption: "New Impure main application"}
```scala
def main(args: Array[String]): Unit = {
  implicit val system: ActorSystem    = ActorSystem()
  implicit val mat: ActorMaterializer = ActorMaterializer()
  implicit val ec: ExecutionContext   = system.dispatcher

  val url = ???
  val user           = ???
  val pass           = ???
  val flyway: Flyway = ???
  val _              = flyway.migrate()

  val dbConfig: DatabaseConfig[JdbcProfile] =
    DatabaseConfig.forConfig("database", system.settings.config)
  val repo = new Repository(dbConfig)

  val productRoutes  = new ProductRoutes(repo)
  val productsRoutes = new ProductsRoutes(repo)
  val routes         = productRoutes.routes ~ productsRoutes.routes

  val host       = system.settings.config.getString("api.host")
  val port       = system.settings.config.getInt("api.port")
  val srv        = Http().bindAndHandle(routes, host, port)
  val pressEnter = StdIn.readLine()
  srv.flatMap(_.unbind()).onComplete(_ => system.terminate())
}
```

We simply create our instances from our routing classes and construct our global routes directly from them. This is good but if we want to test the routes in isolation we still have the problem that they are hard-wired to our `Repository` class which is implemented via Slick. Several options exist to handle this:

1. Use an in-memory test database with according configuration.
2. Abstract further and use a trait instead of the concrete repository implementation.
3. Write integration tests which will require a working database.

Using option 1 is tempting but think about it some more. While the benefit is that we can use our actual implementation and just have to fire up an in-memory database (for example h2), there are also some drawbacks:

1. You have to handle evolutions for the in-memory database.
2. You're evolutions have to be completely portable SQL (read *ANSI SQL*). Otherwise you'll have to write each of your evolutions scripts two times (one for production, one for testing).
3. You're code has to be database agnostic. This sounds easier than it is. Even the tools you're using may use database specific features under the hood.
4. Several features are simply not implemented in some databases. Think of things like cascading deletion via foreign keys.

Taking option 2 is a valid choice but it will result in more code. Also you must pay close attention to the "test repository" implementation to avoid introducing bugs there. Going for the most simple approach is usually feasible. Think of a simple test repository implementation that will just return hard coded values or values passed to it via constructor.

However we will go with option 3 in this case. It has the drawback that you'll have to provide a real database environment (and maybe more) for testing. But it is as close to production as you can get. Also you will need these either way to test your actual repository implementation, so let's get going.

### Integration Tests

First we need to configure our test database because we do not want to accidentally wipe a production database. For our case we leave everything as is and just change the database name.

{caption: "Configuration file for integration tests"}
```text
api {
  host = "localhost"
  port = 49152
}

database {
  profile = "slick.jdbc.PostgresProfile$"
  db {
    connectionPool = "HikariCP"
    dataSourceClass = "org.postgresql.ds.PGSimpleDataSource"
    properties {
      serverName = "localhost"
      portNumber = "5432"
      databaseName = "impure_test"
      user = "impure"
      password = "secret"
    }
    numThreads = 10
  }
}
```

Another thing we should do is provide a test configuration for our logging framework. We use the logback library and Slick will produce a lot of logging output on the `DEBUG` level so we should fix that. It is nice to have logging if you need it but it also clutters up your log files. We create a file `logback-test.xml` in the directory `src/it/resources` which should look like this:

{caption: "Logging configuration for integration tests"}
```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration debug="false">
  <appender name="console" class="ch.qos.logback.core.ConsoleAppender">
    <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
      <level>WARN</level>
    </filter>
    <encoder>
      <pattern>%date %highlight(%-5level) %cyan(%logger{0}) - %msg%n</pattern>
    </encoder>
  </appender>

  <appender name="async-console" class="ch.qos.logback.classic.AsyncAppender">
    <appender-ref ref="console"/>
    <queueSize>5000</queueSize>
    <discardingThreshold>0</discardingThreshold>
  </appender>

  <logger name="com.wegtam.books.pfhais.impure" level="INFO" additivity="false">
    <appender-ref ref="console"/>
  </logger>

  <root>
    <appender-ref ref="console"/>
  </root>
</configuration>
```

Due to the nature of integration tests we want to use production or "production like" settings and environment. But really starting our application or service for each test will be quite cumbersome so we should provide a base class for our tests. In this class we will start our service, migrate our database and provide an opportunity to shut it down properly after testing.

Because we do not want to get into trouble when running an Akka-HTTP on the same port, we first create a helper function which will determine a free port number.

{caption: "Finding a free port number for tests"}
```scala
import java.net.ServerSocket

def findAvailablePort(): Int = {
  val serverSocket = new ServerSocket(0)
  val freePort     = serverSocket.getLocalPort
  serverSocket.setReuseAddress(true)
  serverSocket.close()
  freePort
}
```

The code is quite simple and very useful for such cases. Please not that it is important use `setReuseAddress` because otherwise the found socket will be blocked for a certain amount of time. But now let us continue with our base test class.

{caption: "BaseSpec for integration tests"}
```scala
abstract class BaseSpec
    extends TestKit(
      ActorSystem(
        "it-test",
        ConfigFactory
          .parseString(s"api.port=${BaseSpec.findAvailablePort()}")
          .withFallback(ConfigFactory.load())
      )
    )
    with AsyncWordSpecLike
    with MustMatchers
    with ScalaCheckPropertyChecks
    with BeforeAndAfterAll
    with BeforeAndAfterEach {

  implicit val materializer: ActorMaterializer = ActorMaterializer()

  private val url  = ???
  private val user = ???
  private val pass = ???
  protected val flyway: Flyway = 
    Flyway.configure().dataSource(url, user, pass).load()

  override protected def afterAll(): Unit =
    TestKit.shutdownActorSystem(system, FiniteDuration(5, SECONDS))

  override protected def beforeAll(): Unit = {
    val _ = flyway.migrate()
  }
}
```

As you can see we are using the Akka-Testkit to initialise an actor system. This is useful because there are several helpers available which you might need. We configure the actor system with our free port using the loaded configuration as fallback. Next we globally create an actor materializer which is needed by Akka-HTTP and Akka-Streams. Also we create a globally available Flyway instance to make cleaning and migrating the database easier.
The base class also implements the `beforeAll` and `afterAll` methods which will be run before and after all tests. They are used to initially migrate the database and to shut down the actor system properly in the end.

#### Testing the repository

Now that we our parts in place we can write an integration test for our repository implementation.

I> Please ensure that you have a properly configured database running!

First we need to do some things globally for the test scope.

{caption: "Repository test: global stuff"}
```scala
private val dbConfig: DatabaseConfig[JdbcProfile] =
  DatabaseConfig.forConfig("database", system.settings.config)
private val repo = new Repository(dbConfig)

override protected def beforeEach(): Unit = {
  flyway.clean()
  val _ = flyway.migrate()
  super.beforeEach()
}

override protected def afterEach(): Unit = {
  flyway.clean()
  super.afterEach()
}

override protected def afterAll(): Unit = {
  repo.close()
  super.afterAll()
}
```

We create one `Repository` instance for all our tests here. The downside is that if one test crashes it then the other will be affected too. On the other hand we avoid running into database connection limits and severe code limbo to ensure closing a repository connection after each test no matter the result.
Also we clean and migrate before each test and clean also after each test. This ensures having a clean environment.

D> However, not cleaning your database between tests might be an even more thorough test. ;-)

Onwards to the test for loading a single product.

{caption: "Repository test: loadProduct"}
```scala
"#loadProduct" when {
  "the ID does not exist" must {
    "return an empty list of rows" in {
      val id = UUID.randomUUID
      for {
        rows <- repo.loadProduct(id)
      } yield {
        rows must be(empty)
      }
    }
  }

  "the ID exists" must {
    "return a list with all product rows" in {
      genProduct.sample match {
        case None => fail("Could not generate data sample!")
        case Some(p) =>
          for {
            _    <- repo.saveProduct(p)
            rows <- repo.loadProduct(p.id)
          } yield {
            Product.fromDatabase(rows) match {
              case None => fail("No product created from database rows!")
              case Some(c) =>
                c.id must be(p.id)
                c mustEqual p
            }
          }
      }
    }
  }
}
```

Loading a non existing product must not produce any result and is simple to test if our database is empty. Testing the loading of a real product is not that much more complicated. We use the ScalaCheck generators to create one, save it and load it again. The loaded product must of course be equal to the saved one.

I> You might have noticed that we do not use `forAll` here and thus are testing only one case.
I> This is due an issue which has yet to be fixed[^23] in ScalaTest.

{caption: "Repository test: loadProducts"}
```scala
"#loadProducts" when {
  "no products exist" must {
    "return an empty stream" in {
      val src = Source.fromPublisher(repo.loadProducts())
      for {
        ps <- src.runWith(Sink.seq)
      } yield {
        ps must be(empty)
      }
    }
  }

  "some products exist" must {
    "return a stream with all product rows" in {
      genProducts.sample match {
        case None => fail("Could not generate data sample!")
        case Some(ps) =>
          val expected = ps.flatMap(
          p => p.names.toNonEmptyList.toList.map(
            n => (p.id, n.lang, n.name)
            )
          )
          for {
            _ <- Future.sequence(ps.map(p => repo.saveProduct(p)))
            src = Source
              .fromPublisher(repo.loadProducts())
              // more code omitted here
            rows <- src.runWith(Sink.seq)
          } yield {
            rows must not be (empty)
            rows.size mustEqual ps.size
            rows.toList.sorted mustEqual ps.sorted
          }
      }
    }
  }
}
```

Testing the loading of all products if none exit is trivial like the one for a non existing single product. For the case of multiple products we generate a list of them which we save. Afterwards we load them and use the same transformation logic like in the routes to be able to construct proper `Product` instances. One thing you might notice is the explicit sorting which is due to the fact that we want to ensure that our product lists are both sorted before comparing them.

{caption: "Repository test: saveProduct"}
```scala
"#saveProduct" when {
  "the product does not already exist" must {
    "save the product to the database" in {
      genProduct.sample match {
        case None => fail("Could not generate data sample!")
        case Some(p) =>
          for {
            cnts <- repo.saveProduct(p)
            rows <- repo.loadProduct(p.id)
          } yield {
            withClue("Data missing from database!")(
              cnts.fold(0)(_ + _) must be(p.names.toNonEmptyList.size + 1))
            Product.fromDatabase(rows) match {
              case None => fail("No product created from database rows!")
              case Some(c) =>
                c.id must be(p.id)
                c mustEqual p
            }
          }
      }
    }
  }

  "the product does already exist" must {
    "return an error and not change the database" in {
      (genProduct.sample, genProduct.sample) match {
        case (Some(a), Some(b)) =>
          val p = b.copy(id = a.id)
          for {
            cnts <- repo.saveProduct(a)
            nosv <- repo.saveProduct(p).recover {
              case _ => 0
            }
            rows <- repo.loadProduct(a.id)
          } yield {
            withClue("Saving a duplicate product must fail!")(nosv must be(0))
            Product.fromDatabase(rows) match {
              case None => fail("No product created from database rows!")
              case Some(c) =>
                c.id must be(a.id)
                c mustEqual a
            }
          }
        case _ => fail("Could not create data sample!")
      }
    }
  }
}
```

Here we test the saving which in the first case should simply write the appropriate data into the database. If the product already exists however this should not happen. Our database constraints will ensure that this does not happen (or so we hope ;-)). Slick will throw an exception which we catch in the test code using the `recover` method from `Future` to return a zero indicating no affected database rows. In the end we test for this zero and also check if the originally saved product has not been changed.

{caption: "Repository test: updateProduct"}
```scala
"#updateProduct" when {
  "the product does exist" must {
    "update the database" in {
      (genProduct.sample, genProduct.sample) match {
        case (Some(a), Some(b)) =>
          val p = b.copy(id = a.id)
          for {
            cnts <- repo.saveProduct(a)
            upds <- repo.updateProduct(p)
            rows <- repo.loadProduct(a.id)
          } yield {
            withClue("Already existing product was not created!")(
              cnts.fold(0)(_ + _) must be(a.names.toNonEmptyList.size + 1)
            )
            Product.fromDatabase(rows) match {
              case None => fail("No product created from database rows!")
              case Some(c) =>
                c.id must be(a.id)
                c mustEqual p
            }
          }
        case _ => fail("Could not create data sample!")
      }
    }
  }

  "the product does not exist" must {
    "return an error and not change the database" in {
      genProduct.sample match {
        case None => fail("Could not generate data sample!")
        case Some(p) =>
          for {
            nosv <- repo.updateProduct(p).recover {
              case _ => 0
            }
            rows <- repo.loadProduct(p.id)
          } yield {
            withClue("Updating a not existing product must fail!")
              (nosv must be(0))
            withClue("Product must not exist in database!")
              (rows must be(empty))
          }
      }
    }
  }
}
```

For testing an update we generate two samples, save one to the database, change the id of the other to the one from the first and execute an update. This update should proceed without problems and the data in the database must have been changed correctly.
If the product does not exist then we use the same `recover` technique like in the `saveProduct` test.

Congratulations, we have made a check mark on our first integration test using a real database using randomly generated data!

## Testing the pure service

### Unit Tests

### Integration Tests

# Adding benchmarks

TODO

# Comparison

TODO

[^10]: http://www.scalacheck.org/

[^11]: http://slick.lightbend.com/doc/3.3.1/database.html

[^12]: https://pureconfig.github.io/

[^13]: https://typelevel.org/cats-effect/typeclasses/sync.html

[^14]: http://slick.lightbend.com/doc/3.3.1/sql.html

[^15]: https://blog.acolyer.org/2019/07/03/one-sql-to-rule-them-all/

[^16]: https://httpie.org/

[^17]: https://github.com/http4s/http4s/issues/2371

[^18]: https://www.wartremover.org/

[^19]: https://typelevel.org/cats-effect/datatypes/ioapp.html

[^20]: http://www.scalacheck.org/

[^21]: http://www.scalatest.org/user_guide/selecting_a_style

[^22]: https://rspec.info/

[^23]: https://github.com/scalatest/scalatest/issues/1370
